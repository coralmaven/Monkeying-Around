{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import locale\n",
    "from config import email, password\n",
    "# Import SQL Alchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import and establish Base for which classes will be constructed \n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "# Import modules to declare columns and column data types\n",
    "from sqlalchemy import Column, Integer, String, Float\n",
    "from sqlalchemy.orm import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for scraping\n",
    "class scrape():\n",
    "    def __init__(self, search):\n",
    "        print(\"scraping\")\n",
    "    def all_test(self, search):\n",
    "        # Create an engine for the chinook.sqlite database\n",
    "        engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "        # Declare a Base using `automap_base()`\n",
    "        Base = automap_base()\n",
    "        # Use the Base class to reflect the database tables\n",
    "        Base.prepare(engine, reflect=True)\n",
    "        # Base.metadata.create_all(engine)\n",
    "        # create conn\n",
    "        conn = engine.connect()\n",
    "        # To push the objects made and query the server we use a Session object\n",
    "        session = Session(bind=engine)\n",
    "        search_df = pd.read_sql(\"SELECT * FROM search\", conn)\n",
    "        searched_terms = search_df.search_term.unique()\n",
    "        new_terms = search\n",
    "        old_terms = []\n",
    "        # check to see if data is already in database\n",
    "        list_len = int(len(search))\n",
    "        search_len = int(len(searched_terms))\n",
    "        print(len(search)-1)\n",
    "        print(len(searched_terms)-1)\n",
    "        # check to see if data is already in database\n",
    "        for i in range(0, list_len):\n",
    "            for j in range(0, search_len):\n",
    "                if search[i] == searched_terms[j]:\n",
    "                    old_terms.append(searched_terms[j])\n",
    "                    x = search[i]\n",
    "                else:\n",
    "                    pass\n",
    "        for n in old_terms:\n",
    "            new_terms.remove(n)\n",
    "        if len(new_terms) > 0:\n",
    "            amz = amazon_h10(old_terms)\n",
    "            list_df = amz.mix(new_terms, old_terms)\n",
    "            print(list_df)\n",
    "            return list_df\n",
    "        else:\n",
    "            print(old_terms)\n",
    "            amz = amazon_h10(old_terms)\n",
    "            list_df = amz.old(old_terms)\n",
    "            return list_df\n",
    "\n",
    "\n",
    "class amazon_h10():\n",
    "    def __init__(self, search):\n",
    "        print(f'You searched for {search}')\n",
    "\n",
    "    def complete_scrape(self, search):\n",
    "        print('you are scraping amazon')\n",
    "        amazon_url = \"https://www.amazon.com\"\n",
    "        # open the driver\n",
    "        options = Options()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        # scrape amazon      \n",
    "        search_df = pd.DataFrame()\n",
    "        search_df1 = pd.DataFrame()\n",
    "        amazon_df = pd.DataFrame()  \n",
    "        for word in search:\n",
    "            search_df1['search_term'] = [word]\n",
    "            search_df = search_df.append(search_df1)\n",
    "            try:\n",
    "                search_url = f'https://www.amazon.com/s?k={word}&ref=nb_sb_noss_1'\n",
    "                driver.get(search_url)\n",
    "                html = driver.page_source\n",
    "                search_beautify = BeautifulSoup(html, 'html.parser')\n",
    "                data = search_beautify.findAll('div', class_=\"a-section a-spacing-none\")\n",
    "                try:\n",
    "                    link = [link.a['href'] for link in data]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data2 = search_beautify.findAll('span', class_=\"a-offscreen\")\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    amz_prod_price = [x.text.split('$')[1] for x in data2]\n",
    "                except:\n",
    "                    pass\n",
    "                amz_prod_price = amz_prod_price[:10]\n",
    "                amz_prod_name = []\n",
    "\n",
    "                for i in range(12,22):\n",
    "                    product_link = amazon_url + link[i]\n",
    "                    break_link = product_link.split('/')\n",
    "                    title = break_link[3].replace('-', ' ')\n",
    "                    amz_prod_name.append(title)\n",
    "                amazon_df1 = pd.DataFrame()  \n",
    "                amazon_df1['product_name'] = amz_prod_name\n",
    "                amazon_df1['product_price'] = amz_prod_price\n",
    "                amazon_df1['search_term'] = [f'{word}' for count in amz_prod_name]\n",
    "                amazon_df = amazon_df.append(amazon_df1)\n",
    "            except:\n",
    "                pass\n",
    "#     ==================================\n",
    "        print('you are scraping helium')\n",
    "        # sign into Helium 10\n",
    "        driver.get('https://members.helium10.com/user/signin')\n",
    "        login = driver.find_element_by_id('loginform-email')\n",
    "        login.send_keys(email)\n",
    "        password_route = driver.find_element_by_id('loginform-password')\n",
    "        password_route.send_keys(password)\n",
    "        login_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Log In')]\")\n",
    "        login_button[0].click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # create global data frame\n",
    "        overview_data_df = pd.DataFrame()\n",
    "        top5_monthly_revenue_df = pd.DataFrame()\n",
    "        overview_data_df1 = pd.DataFrame()\n",
    "        top5_monthly_revenue_df1 = pd.DataFrame()\n",
    "        \n",
    "        # type in the key words search\n",
    "        for words in search:\n",
    "            #  link for niche\n",
    "            h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "            # visit link\n",
    "            driver.get(h_10_niche)\n",
    "            search_bar = driver.find_element_by_id('filter-asin')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(words)\n",
    "            \n",
    "            try:\n",
    "                # click on search button\n",
    "                time.sleep(1)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "\n",
    "            # Scrape overview data    \n",
    "            try: \n",
    "                overview_data = driver.find_elements_by_xpath(\"//div[@class='col-xs-12 col-sm-6 col-md-2']\")\n",
    "\n",
    "                col1 = []\n",
    "                col2 = []\n",
    "                time.sleep(1)\n",
    "                for i in range(0,5):\n",
    "                    h_10_data = overview_data[i].text\n",
    "                    h_data = h_10_data.split('\\n')\n",
    "                    col1.append(h_data[0].strip())\n",
    "                    value = h_data[1].split('$')\n",
    "                    if len(value) > 1:\n",
    "                        value = value[1]\n",
    "                        col2.append(value)\n",
    "                    else:\n",
    "                        col2.append(value[0])\n",
    "                # pull data from 20-24\n",
    "                overview_data_df1['revenue_specs'] = col1\n",
    "                overview_data_df1['revenue_value'] = col2\n",
    "                overview_data_df1['search_term'] = [f'{words}' for x in col1]\n",
    "                overview_data_df = overview_data_df.append(overview_data_df1)\n",
    "\n",
    "                # Pull revenue data  \n",
    "                sort_product_list = Select(driver.find_element_by_id('sort'))\n",
    "                driver.find_element_by_xpath(\"//select[@name='sort']/option[text()='Monthly Revenue High To Low']\").click()\n",
    "\n",
    "                # pull in data\n",
    "                # product title\n",
    "                time.sleep(1)\n",
    "                product_name = driver.find_elements_by_class_name('media-heading')\n",
    "                product_names = [name.text for name in product_name]\n",
    "\n",
    "                # price\n",
    "                product_price = driver.find_elements_by_class_name('price-chart')\n",
    "                product_prices = [price.text.split('$')[1] for price in product_price]\n",
    "\n",
    "                # monthly sales\n",
    "                product_sales_monthly = driver.find_elements_by_class_name('monthlySales-column')\n",
    "                product_sales_monthly_s = [sales.text for sales in product_sales_monthly]\n",
    "\n",
    "                # monthly revenue \n",
    "                product_revenue_monthly = driver.find_elements_by_class_name('monthlyRevenue-column')\n",
    "                product_revenue_monthly_s = [rev.text.split('$')[1] for rev in product_revenue_monthly]\n",
    "\n",
    "                # put into a df\n",
    "                top5_monthly_revenue_df1['monthly_product'] = product_names[:5]\n",
    "                top5_monthly_revenue_df1['monthly_price'] = product_prices[:5]\n",
    "                top5_monthly_revenue_df1['monthly_sales'] = product_sales_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['monthly_revenue'] = product_revenue_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['search_term'] = [f'{words}' for x in product_names[:5]]\n",
    "                top5_monthly_revenue_df = top5_monthly_revenue_df.append(top5_monthly_revenue_df1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#     ====================\n",
    "        etsy_df = pd.DataFrame()\n",
    "        etsy_df1 = pd.DataFrame()\n",
    "        print(\"you are scraping etsy\")\n",
    "        # loop over search words        \n",
    "        for word in search:\n",
    "            # visit url\n",
    "            search_url = f'https://www.etsy.com/search?q={word}&explicit=1&order=most_relevant'\n",
    "            driver.get(search_url)\n",
    "            \n",
    "            # product name\n",
    "            try:\n",
    "                product = driver.find_elements_by_class_name(\"v2-listing-card__info\")\n",
    "                product_name = [link.text.strip() for link in product]\n",
    "\n",
    "                # produce price\n",
    "                price = driver.find_elements_by_class_name(\"n-listing-card__price\")\n",
    "                product_price = [x.text.strip() for x in price]\n",
    "                # clean data \n",
    "                n = [x.split('\\n') for x in product_price]\n",
    "                product_price = [y[0] for y in n]\n",
    "                b = [x.split('$') for x in product_price]\n",
    "                locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "                try:\n",
    "                    product_price = [locale.atof(y[1]) for y in b]\n",
    "                except:\n",
    "                    product_price = [y[1] for y in b]\n",
    "\n",
    "                # df to hold the data    \n",
    "                etsy_df1['product_name'] = product_name[:5]\n",
    "                etsy_df1['product_price'] = product_price[:5]\n",
    "                etsy_df1['search_term'] = [f'{word}' for x in product_name[:5]]\n",
    "                etsy_df = etsy_df.append(etsy_df1)\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        print(\"scraping complete\")\n",
    "        search_df = search_df.set_index(['search_term'])\n",
    "        amazon_df = amazon_df.set_index(['search_term'])\n",
    "        overview_data_df = overview_data_df.set_index(['search_term'])\n",
    "        top5_monthly_revenue_df = top5_monthly_revenue_df.set_index(['search_term'])\n",
    "        etsy_df = etsy_df.set_index(['search_term'])\n",
    "        return [search_df, amazon_df, overview_data_df, top5_monthly_revenue_df, etsy_df]\n",
    "\n",
    "    def mix(self, new_terms, old_terms):\n",
    "        # Create an engine for the chinook.sqlite database\n",
    "        engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "        # Declare a Base using `automap_base()`\n",
    "        Base = automap_base()\n",
    "        # Use the Base class to reflect the database tables\n",
    "        Base.prepare(engine, reflect=True)\n",
    "        # Base.metadata.create_all(engine)\n",
    "        # create conn\n",
    "        conn = engine.connect()\n",
    "        # To push the objects made and query the server we use a Session object\n",
    "        session = Session(bind=engine)\n",
    "        search_df = pd.read_sql(\"SELECT * FROM search\", conn)\n",
    "        amazon_df = pd.read_sql(\"SELECT * FROM amazon\", conn)\n",
    "        total_df = pd.read_sql(\"SELECT * FROM total_revenue_h10\", conn)\n",
    "        monthly_df = pd.read_sql(\"SELECT * FROM monthly_revenue_h10\", conn)\n",
    "        etsy_df = pd.read_sql(\"SELECT * FROM etsy\", conn)\n",
    "\n",
    "        search_df = search_df.set_index(['search_term'])\n",
    "        amazon_df = amazon_df.set_index(['search_term'])\n",
    "        total_df = total_df.set_index(['search_term'])\n",
    "        monthly_df = monthly_df.set_index(['search_term'])\n",
    "        etsy_df = etsy_df.set_index(['search_term'])\n",
    "        # scrape for new material\n",
    "        list_df = self.complete_scrape(new_terms)\n",
    "        # separate returned df\n",
    "        search_df_scrape = list_df[0]\n",
    "        amazon_df_scrape = list_df[1]\n",
    "        revenue_df_scrape = list_df[2]\n",
    "        product_data_df_scrape = list_df[3]\n",
    "        etsy_df_scrape = list_df[4]\n",
    "#         search_df_scrape = search_df_scrape.set_index(['search_term'])\n",
    "#         amazon_df_scrape = amazon_df_scrape.set_index(['search_term'])\n",
    "#         revenue_df_scrape = revenue_df_scrape.set_index(['search_term'])\n",
    "#         product_data_df_scrape = product_data_df_scrape.set_index(['search_term'])\n",
    "#         etsy_df_scrape = etsy_df_scrape.set_index(['search_term'])\n",
    "        # put into db\n",
    "        try:\n",
    "            search_df_scrape.to_sql('search', engine, if_exists='append')\n",
    "            amazon_df_scrape.to_sql('amazon', engine, if_exists='append')\n",
    "            revenue_df_scrape.to_sql('total_revenue_h10', engine, if_exists='append')\n",
    "            product_data_df_scrape.to_sql('monthly_revenue_h10', engine, if_exists='append')\n",
    "            etsy_df_scrape.to_sql('etsy', engine, if_exists='append')\n",
    "        except:\n",
    "            pass\n",
    "            print('there was an error when trying to save')\n",
    "        # pull in old material\n",
    "        for word in old_terms:\n",
    "            search_df_scrape = search_df_scrape.append(search_df.loc[word]).reset_index()\n",
    "            amazon_df_scrape = amazon_df_scrape.append(amazon_df.loc[word]).reset_index()\n",
    "            revenue_df_scrape = revenue_df_scrape.append(total_df.loc[word]).reset_index()\n",
    "            product_data_df_scrape = product_data_df_scrape.append(monthly_df.loc[word]).reset_index()\n",
    "            etsy_df_scrape = etsy_df_scrape.append(etsy_df.loc[word]).reset_index()\n",
    "        # parse data into lists\n",
    "#         search_data = {}\n",
    "#         amazon_data = {}\n",
    "#         total_rev_data = {}\n",
    "#         monthly_rev_data = {}\n",
    "#         etsy_data = {}\n",
    "#         terms = old_terms + new_terms\n",
    "#         for phrase in terms:\n",
    "#             # search\n",
    "#             search_data['search_term'] = search_df_scrape['search_term'].values.tolist()\n",
    "#             # amazon\n",
    "#             amazon_data['search_term'] = amazon_df_scrape['search_term'].values.tolist()\n",
    "#             amazon_data['product_name'] = amazon_df_scrape['product_name'].values.tolist()\n",
    "#             amazon_data['product_price'] = amazon_df_scrape['product_price'].values.tolist()\n",
    "#             # helium total revenue\n",
    "#             total_rev_data['search_term'] = revenue_df_scrape['search_term'].values.tolist()\n",
    "#             total_rev_data['revenue_specs'] = revenue_df_scrape['revenue_specs'].values.tolist()\n",
    "#             total_rev_data['revenue_value'] = revenue_df_scrape['revenue_value'].values.tolist()\n",
    "#             # helium monthly revenure by product\n",
    "#             monthly_rev_data['search_term'] = product_data_df_scrape['search_term'].values.tolist()\n",
    "#             monthly_rev_data['monthly_product'] = product_data_df_scrape['monthly_product'].values.tolist()\n",
    "#             monthly_rev_data['monthly_price'] = product_data_df_scrape['monthly_price'].values.tolist()\n",
    "#             monthly_rev_data['monthly_sales'] = product_data_df_scrape['monthly_sales'].values.tolist()\n",
    "#             monthly_rev_data['monthly_revenue'] = product_data_df_scrape['monthly_revenue'].values.tolist()\n",
    "#             # etsy\n",
    "#             etsy_data['search_term'] = etsy_df_scrape['search_term'].values.tolist()\n",
    "#             etsy_data['product_name'] = etsy_df_scrape['product_name'].values.tolist()\n",
    "#             etsy_data['product_price'] = etsy_df_scrape['product_price'].values.tolist()\n",
    "        return [search_df_scrape, amazon_df_scrape, revenue_df_scrape, product_data_df_scrape, etsy_df_scrape]\n",
    "    def old(self, old_terms):\n",
    "        # Create an engine for the chinook.sqlite database\n",
    "        engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "        # Declare a Base using `automap_base()`\n",
    "        Base = automap_base()\n",
    "        # Use the Base class to reflect the database tables\n",
    "        Base.prepare(engine, reflect=True)\n",
    "        # Base.metadata.create_all(engine)\n",
    "        # create conn\n",
    "        conn = engine.connect()\n",
    "        # To push the objects made and query the server we use a Session object\n",
    "        session = Session(bind=engine)\n",
    "        search_df = pd.read_sql(\"SELECT * FROM search\", conn)\n",
    "        amazon_df = pd.read_sql(\"SELECT * FROM amazon\", conn)\n",
    "        total_df = pd.read_sql(\"SELECT * FROM total_revenue_h10\", conn)\n",
    "        monthly_df = pd.read_sql(\"SELECT * FROM monthly_revenue_h10\", conn)\n",
    "        etsy_df = pd.read_sql(\"SELECT * FROM etsy\", conn)\n",
    "        search_df = search_df.set_index(['search_term'])\n",
    "        amazon_df = amazon_df.set_index(['search_term'])\n",
    "        total_df = total_df.set_index(['search_term'])\n",
    "        monthly_df = monthly_df.set_index(['search_term'])\n",
    "        etsy_df = etsy_df.set_index(['search_term'])\n",
    "        \n",
    "#         .set_index(['search_term'])\n",
    "        search_req = pd.DataFrame()\n",
    "        amazon_req = pd.DataFrame()\n",
    "        revenue_req = pd.DataFrame()\n",
    "        product_data_req = pd.DataFrame()\n",
    "        etsy_req = pd.DataFrame()\n",
    "        for word in old_terms:\n",
    "            search_req = search_req.append(search_df.loc[word])\n",
    "            amazon_req = amazon_req.append(amazon_df.loc[word])\n",
    "            revenue_req = revenue_req.append(total_df.loc[word])\n",
    "            product_data_req = product_data_req.append(monthly_df.loc[word])\n",
    "            etsy_req = etsy_req.append(etsy_df.loc[word])\n",
    "#         \n",
    "#         for phrase in old_terms:\n",
    "#             # search\n",
    "# #             search_data['search_term'] = search_req['search_term'].values.tolist()\n",
    "#             # amazon\n",
    "#             amazon_data['search_term'] = amazon_req['search_term'].values.tolist()\n",
    "#             amazon_data['product_name'] = amazon_req['product_name'].values.tolist()\n",
    "#             amazon_data['product_price'] = amazon_req['product_price'].values.tolist()\n",
    "#             # helium total revenue\n",
    "#             total_rev_data['search_term'] = revenue_req['search_term'].values.tolist()\n",
    "#             total_rev_data['revenue_specs'] = revenue_req['revenue_specs'].values.tolist()\n",
    "#             total_rev_data['revenue_value'] = revenue_req['revenue_value'].values.tolist()\n",
    "#             # helium monthly revenure by product\n",
    "#             monthly_rev_data['search_term'] = product_data_req['search_term'].values.tolist()\n",
    "#             monthly_rev_data['monthly_product'] = product_data_req['monthly_product'].values.tolist()\n",
    "#             monthly_rev_data['monthly_price'] = product_data_req['monthly_price'].values.tolist()\n",
    "#             monthly_rev_data['monthly_sales'] = product_data_req['monthly_sales'].values.tolist()\n",
    "#             monthly_rev_data['monthly_revenue'] = product_data_req['monthly_revenue'].values.tolist()\n",
    "#             # etsy\n",
    "#             etsy_data['search_term'] = etsy_req['search_term'].values.tolist()\n",
    "#             etsy_data['product_name'] = etsy_req['product_name'].values.tolist()\n",
    "#             etsy_data['product_price'] = etsy_req['product_price'].values.tolist()\n",
    "        return [search_req, amazon_req, revenue_req, product_data_req, etsy_req]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['candy', 'chocolate', 'lollipop', 'bananas', 'oranges', 'apples',\n",
       "       'tacos', 'kebabs', 'sandwiches', 'energy drink', 'popcorn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = ['candy', 'chocolate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'candy, chocolate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candy', 'chocolate']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = test.split(',')\n",
    "x = [b.strip() for b in x]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping\n",
      "1\n",
      "12\n",
      "You searched for ['candy']\n",
      "you are scraping amazon\n",
      "you are scraping helium\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=74.0.3729.169)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Mac OS X 10.14.3 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-cd1f5d369910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-210-e47d274d4acb>\u001b[0m in \u001b[0;36mall_test\u001b[0;34m(self, search)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_terms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mamz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamazon_h10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mlist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-e47d274d4acb>\u001b[0m in \u001b[0;36mmix\u001b[0;34m(self, new_terms, old_terms)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0metsy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metsy_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search_term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# scrape for new material\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mlist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;31m# separate returned df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0msearch_df_scrape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-e47d274d4acb>\u001b[0m in \u001b[0;36mcomplete_scrape\u001b[0;34m(self, search)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'you are scraping helium'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# sign into Helium 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://members.helium10.com/user/signin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mlogin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loginform-email'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=74.0.3729.169)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Mac OS X 10.14.3 x86_64)\n"
     ]
    }
   ],
   "source": [
    "test = scrape(x)\n",
    "dfs = test.all_test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>AirHeads Chewy Variety Halloween Packaging</td>\n",
       "      <td>13.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Count PATCH Sour Individually Wrapped</td>\n",
       "      <td>10.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Your Favorite Party Brand Candy</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Haribo Gold Bears Delicious Packaging Packaging</td>\n",
       "      <td>14.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Favorite Premium Skittles Tootsie Packed</td>\n",
       "      <td>11.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Assorted Frooties Candy 3 Lb</td>\n",
       "      <td>10.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Skittles Swedish Twizzlers Starburst Variety</td>\n",
       "      <td>10.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>JOLLY RANCHER Holiday Candy Assortment</td>\n",
       "      <td>79.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>MS Halloween Variety 33 08 Ounce 60 Piece</td>\n",
       "      <td>18.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>SKITTLES STARBURST Christmas Variety 30 Count</td>\n",
       "      <td>12.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Chocolate Truffles Easter Candy 5 31 Ounce</td>\n",
       "      <td>8.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Assortment European Chocolate Assorted Individ...</td>\n",
       "      <td>12.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>HERSHEYS Nuggets Chocolate Candy Assortment</td>\n",
       "      <td>16.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Chocolate English Subtitled JeeJa Yanin</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>HERSHEYS Miniatures Chocolate Candy Assortment</td>\n",
       "      <td>23.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Chocolatier Classic Ballotin Chocolate Perfect</td>\n",
       "      <td>8.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Lindt Assorted Chocolate Gourmet Truffles</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Pepperidge Farm Chocolate Multi pack 20 count</td>\n",
       "      <td>11.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>MS Halloween Variety 33 08 Ounce 60 Piece</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>HERSH%C3%89YS Hersheys Chocolate Variety Reeses</td>\n",
       "      <td>8.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  product_name product_price\n",
       "search_term                                                                 \n",
       "candy               AirHeads Chewy Variety Halloween Packaging         13.99\n",
       "candy                    Count PATCH Sour Individually Wrapped         10.98\n",
       "candy                          Your Favorite Party Brand Candy         19.99\n",
       "candy          Haribo Gold Bears Delicious Packaging Packaging         14.99\n",
       "candy                 Favorite Premium Skittles Tootsie Packed         11.84\n",
       "candy                             Assorted Frooties Candy 3 Lb         10.98\n",
       "candy             Skittles Swedish Twizzlers Starburst Variety         10.97\n",
       "candy                   JOLLY RANCHER Holiday Candy Assortment         79.20\n",
       "candy                MS Halloween Variety 33 08 Ounce 60 Piece         18.99\n",
       "candy            SKITTLES STARBURST Christmas Variety 30 Count         12.79\n",
       "chocolate           Chocolate Truffles Easter Candy 5 31 Ounce          8.98\n",
       "chocolate    Assortment European Chocolate Assorted Individ...         12.99\n",
       "chocolate          HERSHEYS Nuggets Chocolate Candy Assortment         16.99\n",
       "chocolate              Chocolate English Subtitled JeeJa Yanin         19.99\n",
       "chocolate       HERSHEYS Miniatures Chocolate Candy Assortment         23.99\n",
       "chocolate       Chocolatier Classic Ballotin Chocolate Perfect          8.61\n",
       "chocolate            Lindt Assorted Chocolate Gourmet Truffles          4.98\n",
       "chocolate        Pepperidge Farm Chocolate Multi pack 20 count         11.97\n",
       "chocolate            MS Halloween Variety 33 08 Ounce 60 Piece          2.99\n",
       "chocolate      HERSH%C3%89YS Hersheys Chocolate Variety Reeses          8.96"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine for the chinook.sqlite database\n",
    "engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "# Declare a Base using `automap_base()`\n",
    "Base = automap_base()\n",
    "# Use the Base class to reflect the database tables\n",
    "Base.prepare(engine, reflect=True)\n",
    "# Base.metadata.create_all(engine)\n",
    "# create conn\n",
    "conn = engine.connect()\n",
    "# To push the objects made and query the server we use a Session object\n",
    "session = Session(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = pd.read_sql(\"SELECT * FROM search\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_df\n",
    "\n",
    "# Create an engine for the chinook.sqlite database\n",
    "engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "# Declare a Base using `automap_base()`\n",
    "Base = automap_base()\n",
    "# Use the Base class to reflect the database tables\n",
    "Base.prepare(engine, reflect=True)\n",
    "# Base.metadata.create_all(engine)\n",
    "# create conn\n",
    "conn = engine.connect()\n",
    "# To push the objects made and query the server we use a Session object\n",
    "session = Session(bind=engine)\n",
    "search_df = pd.read_sql(\"SELECT * FROM search\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_df = pd.read_sql(\"SELECT * FROM search\", conn)\n",
    "# searched_terms = search_df.search_term.unique()\n",
    "# new_terms = search\n",
    "# old_terms = []\n",
    "# # check to see if data is already in database\n",
    "# for i in range(0, (len(keywords)-1)):\n",
    "#     for j in range(0, (len(searched_terms)-1)):\n",
    "#         if keywords[i] == searched_terms[j]:\n",
    "#             old_terms.append(keywords[i])\n",
    "#             del new_terms[i]\n",
    "#         else:\n",
    "#             pass\n",
    "# read in all tables\n",
    "search_df = pd.read_sql(\"SELECT * FROM search\", conn)\n",
    "amazon_df = pd.read_sql(\"SELECT * FROM amazon\", conn)\n",
    "total_df = pd.read_sql(\"SELECT * FROM total_revenue_h10\", conn)\n",
    "monthly_df = pd.read_sql(\"SELECT * FROM monthly_revenue_h10\", conn)\n",
    "etsy_df = pd.read_sql(\"SELECT * FROM etsy\", conn)\n",
    "# if len(new_terms) > 0:\n",
    "#     self.mix(new_terms, old_terms)\n",
    "# else:\n",
    "#     self.old(old_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all tables\n",
    "search_df = pd.read_sql(\"SELECT * FROM search\", conn).set_index(['search_term'])\n",
    "amazon_df = pd.read_sql(\"SELECT * FROM amazon\", conn)\n",
    "total_df = pd.read_sql(\"SELECT * FROM total_revenue_h10\", conn)\n",
    "monthly_df = pd.read_sql(\"SELECT * FROM monthly_revenue_h10\", conn)\n",
    "etsy_df = pd.read_sql(\"SELECT * FROM etsy\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = search_df.set_index(['search_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['candy', 'energy drink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df.loc[keywords[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df\n",
    "search_new_df = pd.DataFrame()\n",
    "amazon_new_df = pd.DataFrame()\n",
    "total_new_df = pd.DataFrame()\n",
    "monthly_new_df = pd.DataFrame()\n",
    "etsy_new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_terms) > 0:\n",
    "    self.mix(new_terms, old_terms)\n",
    "else:\n",
    "    self.old(old_terms)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
